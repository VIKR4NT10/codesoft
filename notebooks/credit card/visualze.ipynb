{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6fab699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f1aae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv('fraudTrain.csv', index_col=0)\n",
    "# test_df = pd.read_csv('fraudTest.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0328d537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)\n",
    "# fraud_counts = df['is_fraud'].value_counts()\n",
    "# print(fraud_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888a8feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8, 8))\n",
    "# labels = ['Legitimate (0)', 'Fraud (1)']\n",
    "# colors = ['#66b3ff', '#ff9999']\n",
    "# explode = (0, 0.1)  \n",
    "# plt.pie(fraud_counts, \n",
    "#         labels=labels, \n",
    "#         autopct='%1.1f%%', \n",
    "#         startangle=140, \n",
    "#         colors=colors, \n",
    "#         explode=explode, \n",
    "#         shadow=True)\n",
    "\n",
    "# plt.title('Distribution of Fraudulent vs Legitimate Transactions', fontsize=15)\n",
    "# plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c1d961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Value Counts:\")\n",
    "# print(fraud_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c84b8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('cc_data.csv', index=False)\n",
    "# print(\"File saved successfully as 'combined_fraud_data.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6881a04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cc_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b5a0c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb220961",
   "metadata": {},
   "source": [
    "2Ô∏è‚É£ Feature engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37c1f546",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'])\n",
    "df['dob'] = pd.to_datetime(df['dob'])\n",
    "\n",
    "# Sort by customer and time\n",
    "df = df.sort_values(by=['cc_num', 'trans_date_trans_time']).reset_index(drop=True)\n",
    "\n",
    "# Time features\n",
    "df['transaction_hour'] = df['trans_date_trans_time'].dt.hour\n",
    "df['transaction_day'] = df['trans_date_trans_time'].dt.day\n",
    "df['transaction_month'] = df['trans_date_trans_time'].dt.month\n",
    "df['transaction_weekday'] = df['trans_date_trans_time'].dt.weekday\n",
    "df['is_weekend'] = df['transaction_weekday'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Age\n",
    "df['age'] = (df['trans_date_trans_time'] - df['dob']).dt.days / 365.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54bc723a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Rolling transaction counts\n",
    "# -------------------------------\n",
    "df.set_index('trans_date_trans_time', inplace=True)\n",
    "df['txn_count_1h'] = df.groupby('cc_num')['amt'].rolling('1h').count().reset_index(level=0, drop=True)\n",
    "df['txn_count_24h'] = df.groupby('cc_num')['amt'].rolling('24h').count().reset_index(level=0, drop=True)\n",
    "\n",
    "# Rolling amount statistics (24h, no leakage)\n",
    "df['amt_mean_24h'] = df.groupby('cc_num')['amt'].rolling('24h', closed='left').mean().reset_index(level=0, drop=True)\n",
    "df['amt_std_24h'] = df.groupby('cc_num')['amt'].rolling('24h', closed='left').std().reset_index(level=0, drop=True)\n",
    "df['amt_zscore_24h'] = (df['amt'] - df['amt_mean_24h']) / (df['amt_std_24h'] + 1e-6)\n",
    "df.reset_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a79cc62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Geodistance\n",
    "# -------------------------------\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2)**2\n",
    "    c = 2*np.arcsin(np.sqrt(a))\n",
    "    return 6371 * c\n",
    "\n",
    "df['geo_distance_km'] = haversine(df['lat'], df['long'], df['merch_lat'], df['merch_long'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad0652d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------\n",
    "# Time-based train/test split (no leakage)\n",
    "# -------------------------------\n",
    "df = df.sort_values('trans_date_trans_time')\n",
    "split_time = df['trans_date_trans_time'].quantile(0.8)\n",
    "train_df = df[df['trans_date_trans_time'] <= split_time].copy()\n",
    "test_df  = df[df['trans_date_trans_time'] > split_time].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f43fe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 6 Handle missing values safely\n",
    "# -------------------------------\n",
    "cols_to_process = ['amt_mean_24h', 'amt_std_24h', 'amt_zscore_24h']\n",
    "for col in cols_to_process:\n",
    "    train_df[f'{col}_missing'] = train_df[col].isna().astype(int)\n",
    "    test_df[f'{col}_missing'] = test_df[col].isna().astype(int)\n",
    "    median = train_df[col].median()  # Use train median only\n",
    "    train_df[col] = train_df[col].fillna(median)\n",
    "    test_df[col] = test_df[col].fillna(median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f65ec001",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------\n",
    "# Risk encoding (train-only, no leakage)\n",
    "# -------------------------------\n",
    "global_fraud_rate = train_df['is_fraud'].mean()\n",
    "\n",
    "def risk_encode(train_df, test_df, col, target='is_fraud', min_samples=50):\n",
    "    stats = train_df.groupby(col)[target].agg(['mean','count']).rename(columns={'mean':'fraudrate','count':'n'})\n",
    "    stats['risk'] = (stats['fraudrate']*stats['n'] + global_fraud_rate*min_samples) / (stats['n']+min_samples)\n",
    "    train_encoded = train_df[col].map(stats['risk']).fillna(global_fraud_rate)\n",
    "    test_encoded  = test_df[col].map(stats['risk']).fillna(global_fraud_rate)\n",
    "    return train_encoded, test_encoded\n",
    "\n",
    "for col in ['merchant','category','job']:\n",
    "    train_df[f'{col}_risk'], test_df[f'{col}_risk'] = risk_encode(train_df, test_df, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc97f8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['trans_date_trans_time', 'cc_num', 'merchant', 'category', 'amt', 'first', 'last', 'gender', 'street', 'city', 'state', 'zip', 'lat', 'long', 'city_pop', 'job', 'dob', 'trans_num', 'unix_time', 'merch_lat', 'merch_long', 'is_fraud', 'transaction_hour', 'transaction_day', 'transaction_month', 'transaction_weekday', 'is_weekend', 'age', 'txn_count_1h', 'txn_count_24h', 'amt_mean_24h', 'amt_std_24h', 'amt_zscore_24h', 'geo_distance_km', 'amt_mean_24h_missing', 'amt_std_24h_missing', 'amt_zscore_24h_missing', 'merchant_risk', 'category_risk', 'job_risk']\n"
     ]
    }
   ],
   "source": [
    "print(train_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5457a8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 8 Prepare final features\n",
    "# -------------------------------\n",
    "FINAL_FEATURES = [\n",
    "    'amt', 'gender', 'city_pop', 'age', 'transaction_hour', 'transaction_day', \n",
    "    'transaction_month', 'transaction_weekday', 'is_weekend',\n",
    "    'txn_count_1h', 'txn_count_24h', 'amt_mean_24h', 'amt_std_24h', 'amt_zscore_24h',\n",
    "    'amt_mean_24h_missing', 'amt_std_24h_missing', 'amt_zscore_24h_missing',\n",
    "    'geo_distance_km', 'merchant_risk', 'category_risk', 'job_risk',\n",
    "    'lat', 'long', 'merch_lat', 'merch_long'\n",
    "]\n",
    "\n",
    "\n",
    "X_train = train_df[FINAL_FEATURES].copy()\n",
    "y_train = train_df['is_fraud']\n",
    "X_test  = test_df[FINAL_FEATURES].copy()\n",
    "y_test  = test_df['is_fraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2674c4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------\n",
    "# Encode gender\n",
    "# -------------------------------\n",
    "X_train['gender'] = X_train['gender'].str.upper().map({'M':1,'F':0}).fillna(-1).astype(int)\n",
    "X_test['gender']  = X_test['gender'].str.upper().map({'M':1,'F':0}).fillna(-1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d44481a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amt</th>\n",
       "      <th>gender</th>\n",
       "      <th>city_pop</th>\n",
       "      <th>age</th>\n",
       "      <th>transaction_hour</th>\n",
       "      <th>transaction_day</th>\n",
       "      <th>transaction_month</th>\n",
       "      <th>transaction_weekday</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>txn_count_1h</th>\n",
       "      <th>txn_count_24h</th>\n",
       "      <th>amt_mean_24h</th>\n",
       "      <th>amt_std_24h</th>\n",
       "      <th>amt_zscore_24h</th>\n",
       "      <th>amt_mean_24h_missing</th>\n",
       "      <th>amt_std_24h_missing</th>\n",
       "      <th>amt_zscore_24h_missing</th>\n",
       "      <th>geo_distance_km</th>\n",
       "      <th>merchant_risk</th>\n",
       "      <th>category_risk</th>\n",
       "      <th>job_risk</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>merch_lat</th>\n",
       "      <th>merch_long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>839573</th>\n",
       "      <td>4.97</td>\n",
       "      <td>0</td>\n",
       "      <td>3495</td>\n",
       "      <td>30.814511</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>54.852361</td>\n",
       "      <td>41.299177</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>78.597568</td>\n",
       "      <td>0.013787</td>\n",
       "      <td>0.013851</td>\n",
       "      <td>0.004182</td>\n",
       "      <td>36.0788</td>\n",
       "      <td>-81.1781</td>\n",
       "      <td>36.011293</td>\n",
       "      <td>-82.048315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68160</th>\n",
       "      <td>107.23</td>\n",
       "      <td>0</td>\n",
       "      <td>149</td>\n",
       "      <td>40.531143</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>54.852361</td>\n",
       "      <td>41.299177</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.212176</td>\n",
       "      <td>0.010485</td>\n",
       "      <td>0.013605</td>\n",
       "      <td>0.001919</td>\n",
       "      <td>48.8878</td>\n",
       "      <td>-118.2105</td>\n",
       "      <td>49.159047</td>\n",
       "      <td>-118.186462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443631</th>\n",
       "      <td>220.11</td>\n",
       "      <td>1</td>\n",
       "      <td>4154</td>\n",
       "      <td>56.950034</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>54.852361</td>\n",
       "      <td>41.299177</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>108.206083</td>\n",
       "      <td>0.001931</td>\n",
       "      <td>0.002415</td>\n",
       "      <td>0.012877</td>\n",
       "      <td>42.1808</td>\n",
       "      <td>-112.2620</td>\n",
       "      <td>43.150704</td>\n",
       "      <td>-112.154481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974884</th>\n",
       "      <td>45.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1939</td>\n",
       "      <td>51.969884</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>54.852361</td>\n",
       "      <td>41.299177</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>95.673231</td>\n",
       "      <td>0.003075</td>\n",
       "      <td>0.004446</td>\n",
       "      <td>0.006886</td>\n",
       "      <td>46.2306</td>\n",
       "      <td>-112.1138</td>\n",
       "      <td>47.034331</td>\n",
       "      <td>-112.561071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702664</th>\n",
       "      <td>41.96</td>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>32.763860</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>54.852361</td>\n",
       "      <td>41.299177</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>77.556744</td>\n",
       "      <td>0.003858</td>\n",
       "      <td>0.003093</td>\n",
       "      <td>0.005571</td>\n",
       "      <td>38.4207</td>\n",
       "      <td>-79.4629</td>\n",
       "      <td>38.674999</td>\n",
       "      <td>-78.632459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           amt  gender  city_pop        age  transaction_hour  \\\n",
       "839573    4.97       0      3495  30.814511                 0   \n",
       "68160   107.23       0       149  40.531143                 0   \n",
       "443631  220.11       1      4154  56.950034                 0   \n",
       "974884   45.00       1      1939  51.969884                 0   \n",
       "702664   41.96       1        99  32.763860                 0   \n",
       "\n",
       "        transaction_day  transaction_month  transaction_weekday  is_weekend  \\\n",
       "839573                1                  1                    1           0   \n",
       "68160                 1                  1                    1           0   \n",
       "443631                1                  1                    1           0   \n",
       "974884                1                  1                    1           0   \n",
       "702664                1                  1                    1           0   \n",
       "\n",
       "        txn_count_1h  txn_count_24h  amt_mean_24h  amt_std_24h  \\\n",
       "839573           1.0            1.0     54.852361    41.299177   \n",
       "68160            1.0            1.0     54.852361    41.299177   \n",
       "443631           1.0            1.0     54.852361    41.299177   \n",
       "974884           1.0            1.0     54.852361    41.299177   \n",
       "702664           1.0            1.0     54.852361    41.299177   \n",
       "\n",
       "        amt_zscore_24h  amt_mean_24h_missing  amt_std_24h_missing  \\\n",
       "839573          -0.298                     0                    0   \n",
       "68160           -0.298                     0                    0   \n",
       "443631          -0.298                     0                    0   \n",
       "974884          -0.298                     0                    0   \n",
       "702664          -0.298                     0                    0   \n",
       "\n",
       "        amt_zscore_24h_missing  geo_distance_km  merchant_risk  category_risk  \\\n",
       "839573                       0        78.597568       0.013787       0.013851   \n",
       "68160                        0        30.212176       0.010485       0.013605   \n",
       "443631                       0       108.206083       0.001931       0.002415   \n",
       "974884                       0        95.673231       0.003075       0.004446   \n",
       "702664                       0        77.556744       0.003858       0.003093   \n",
       "\n",
       "        job_risk      lat      long  merch_lat  merch_long  \n",
       "839573  0.004182  36.0788  -81.1781  36.011293  -82.048315  \n",
       "68160   0.001919  48.8878 -118.2105  49.159047 -118.186462  \n",
       "443631  0.012877  42.1808 -112.2620  43.150704 -112.154481  \n",
       "974884  0.006886  46.2306 -112.1138  47.034331 -112.561071  \n",
       "702664  0.005571  38.4207  -79.4629  38.674999  -78.632459  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1001aad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.under_sampling import TomekLinks, EditedNearestNeighbours\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f50f99e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as VIKR4NT10\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Accessing as VIKR4NT10\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"VIKR4NT10/codesoft\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"VIKR4NT10/codesoft\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository VIKR4NT10/codesoft initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository VIKR4NT10/codesoft initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/c9b9923bb97249ce941d5332da26ec05', creation_time=1768716939875, experiment_id='4', last_update_time=1768716939875, lifecycle_stage='active', name='Fast_Model_Selection for Credit Card Fraud Detection', tags={}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ========================== CONFIG ==========================\n",
    "CONFIG = {\n",
    "    \"experiment_name\": \"Fast_Model_Selection for Credit Card Fraud Detection\",\n",
    "    \"mlflow_uri\": \"https://dagshub.com/VIKR4NT10/codesoft.mlflow\",\n",
    "    \"repo_owner\": \"VIKR4NT10\",\n",
    "    \"repo_name\": \"codesoft\"\n",
    "}\n",
    "\n",
    "# ========================== MLflow + DAGsHub ==========================\n",
    "mlflow.set_tracking_uri(CONFIG[\"mlflow_uri\"])\n",
    "dagshub.init(\n",
    "    repo_owner=CONFIG[\"repo_owner\"],\n",
    "    repo_name=CONFIG[\"repo_name\"],\n",
    "    mlflow=True\n",
    ")\n",
    "mlflow.set_experiment(CONFIG[\"experiment_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82576aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X_test, y_test):\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "   \n",
    "    return {\n",
    "        \"PR_AUC\": average_precision_score(y_test, y_prob),\n",
    "        \"ROC_AUC\": roc_auc_score(y_test, y_prob)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe8a01c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TREE_FEATURES = X_train.columns.tolist()\n",
    "\n",
    "LINEAR_FEATURES = [\n",
    "    c for c in X_train.columns\n",
    "    if c not in [\"amt_mean_24h\", \"amt_std_24h\"]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d8e9ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"xgb\": XGBClassifier(\n",
    "        n_estimators=100,         # reduced from 300\n",
    "        max_depth=4,              # reduced from 6\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        scale_pos_weight=(y_train == 0).sum() / (y_train == 1).sum(),\n",
    "        tree_method=\"hist\",\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"rf\": RandomForestClassifier(\n",
    "        n_estimators=100,         # reduced from 200\n",
    "        max_depth=10,             # reduced from 18\n",
    "        min_samples_leaf=50,\n",
    "        class_weight=\"balanced\",\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"logreg\": LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        class_weight=\"balanced\",\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    \"mlp\": MLPClassifier(\n",
    "        hidden_layer_sizes=(64, 32),\n",
    "        alpha=1e-4,\n",
    "        max_iter=30,\n",
    "        early_stopping=True,\n",
    "        random_state=42\n",
    "    )\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50acc695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Samplers ----------\n",
    "samplers = {\n",
    "    \"cost_sensitive\": None,\n",
    "    \"tomek\": TomekLinks(n_jobs=-1),\n",
    "    \"enn\": EditedNearestNeighbours(n_neighbors=3, n_jobs=-1),\n",
    "    \"smote_enn\": SMOTEENN(random_state=42)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27e4a8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [\n",
    "    (\"cost_sensitive\", \"xgb\"),\n",
    "    (\"cost_sensitive\", \"rf\"),\n",
    "    (\"cost_sensitive\", \"logreg\"),\n",
    "    (\"tomek\", \"xgb\"),        # will skip TomekLinks automatically\n",
    "    (\"tomek\", \"rf\"),         # will skip TomekLinks automatically\n",
    "    (\"enn\", \"rf\"),\n",
    "    (\"smote_enn\", \"logreg\"),\n",
    "    (\"cost_sensitive\", \"mlp\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c9981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mlflow\n",
    "\n",
    "# # Set your experiment name\n",
    "# mlflow.set_experiment(\"credit_card_fraud_experiments\")\n",
    "\n",
    "# # Get the experiment object\n",
    "# experiment = mlflow.get_experiment_by_name(\"credit_card_fraud_experiments\")\n",
    "# experiment_id = experiment.experiment_id\n",
    "# print(\"Experiment ID:\", experiment_id)\n",
    "# runs = mlflow.search_runs(experiment_ids=[experiment_id])\n",
    "# print(runs[[\"run_id\", \"status\", \"tags.mlflow.runName\"]])\n",
    "# for run_id in runs[\"run_id\"]:\n",
    "#     mlflow.delete_run(run_id)\n",
    "\n",
    "# print(f\"Deleted {len(runs)} runs from experiment '{experiment.name}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be3cba7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "FRAC = 0.3  # 30% of training data\n",
    "\n",
    "X_train_sub, _, y_train_sub, _ = train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    train_size=FRAC,\n",
    "    stratify=y_train,\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4bd5864f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running: cost_sensitive + xgb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/18 16:37:32 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run receptive-horse-47 at: https://dagshub.com/VIKR4NT10/codesoft.mlflow/#/experiments/5/runs/e9ed5b8592b04cdbb27a13acf35b8d15\n",
      "üß™ View experiment at: https://dagshub.com/VIKR4NT10/codesoft.mlflow/#/experiments/5\n",
      "\n",
      "Running: cost_sensitive + rf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/18 16:39:35 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run chill-gull-299 at: https://dagshub.com/VIKR4NT10/codesoft.mlflow/#/experiments/5/runs/9fd3c16c7c884975840e7269df1ceb2b\n",
      "üß™ View experiment at: https://dagshub.com/VIKR4NT10/codesoft.mlflow/#/experiments/5\n",
      "\n",
      "Running: cost_sensitive + logreg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/18 16:40:18 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run adorable-goose-762 at: https://dagshub.com/VIKR4NT10/codesoft.mlflow/#/experiments/5/runs/1b07b372955d436494cbf930654f92f4\n",
      "üß™ View experiment at: https://dagshub.com/VIKR4NT10/codesoft.mlflow/#/experiments/5\n",
      "\n",
      "Running: tomek + xgb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/18 17:01:10 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run handsome-shad-336 at: https://dagshub.com/VIKR4NT10/codesoft.mlflow/#/experiments/5/runs/8baf5fc1e4d34dbdbb94b04f33a34312\n",
      "üß™ View experiment at: https://dagshub.com/VIKR4NT10/codesoft.mlflow/#/experiments/5\n",
      "\n",
      "Running: tomek + rf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/18 17:25:47 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run victorious-swan-781 at: https://dagshub.com/VIKR4NT10/codesoft.mlflow/#/experiments/5/runs/6c1c682ab6154de183e34e1add0cf4b5\n",
      "üß™ View experiment at: https://dagshub.com/VIKR4NT10/codesoft.mlflow/#/experiments/5\n",
      "\n",
      "Running: enn + rf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/18 17:49:47 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run clumsy-wren-362 at: https://dagshub.com/VIKR4NT10/codesoft.mlflow/#/experiments/5/runs/83a76c3a60bd420195e09bbffbeb5318\n",
      "üß™ View experiment at: https://dagshub.com/VIKR4NT10/codesoft.mlflow/#/experiments/5\n",
      "\n",
      "Running: smote_enn + logreg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/18 19:10:41 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run intelligent-grub-994 at: https://dagshub.com/VIKR4NT10/codesoft.mlflow/#/experiments/5/runs/43ea28b351e9419f90774c9491ab3453\n",
      "üß™ View experiment at: https://dagshub.com/VIKR4NT10/codesoft.mlflow/#/experiments/5\n",
      "\n",
      "Running: cost_sensitive + mlp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/18 19:16:59 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run industrious-swan-37 at: https://dagshub.com/VIKR4NT10/codesoft.mlflow/#/experiments/5/runs/a80625b9a27247fb9a12eac92afcd9f6\n",
      "üß™ View experiment at: https://dagshub.com/VIKR4NT10/codesoft.mlflow/#/experiments/5\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Experiment loop ----------------\n",
    "results = []\n",
    "mlflow.set_experiment(\"credit_card_fraud_experiments\")\n",
    "\n",
    "for sampler_name, model_name in experiments:\n",
    "    print(f\"\\nRunning: {sampler_name} + {model_name}\")\n",
    "\n",
    "    sampler = samplers[sampler_name]\n",
    "    model = models[model_name]\n",
    "\n",
    "    # Feature selection\n",
    "    if model_name in [\"xgb\", \"rf\"]:\n",
    "        Xtr, Xte = X_train_sub, X_test\n",
    "        feature_type = \"tree\"\n",
    "        use_scaler = False\n",
    "    else:\n",
    "        Xtr = X_train_sub[LINEAR_FEATURES]\n",
    "        Xte = X_test[LINEAR_FEATURES]\n",
    "        feature_type = \"linear\"\n",
    "        use_scaler = True\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_param(\"model\", model_name)\n",
    "        mlflow.log_param(\"sampler\", sampler_name)\n",
    "        mlflow.log_param(\"feature_type\", feature_type)\n",
    "        mlflow.log_param(\"train_frac\", FRAC)\n",
    "\n",
    "        # ----- Build pipeline -----\n",
    "        steps = []\n",
    "\n",
    "        if sampler is not None:\n",
    "            steps.append((\"sampler\", sampler))\n",
    "\n",
    "        if use_scaler:\n",
    "            steps.append((\"scaler\", StandardScaler()))\n",
    "\n",
    "        steps.append((\"model\", model))\n",
    "\n",
    "        pipeline = Pipeline(steps)\n",
    "\n",
    "        # ----- Train -----\n",
    "        pipeline.fit(Xtr, y_train_sub)\n",
    "\n",
    "        # ----- Evaluate -----\n",
    "        metrics = evaluate(pipeline, Xte, y_test)\n",
    "\n",
    "        for k, v in metrics.items():\n",
    "            mlflow.log_metric(k, v)\n",
    "\n",
    "        mlflow.sklearn.log_model(pipeline, \"model\")\n",
    "\n",
    "        results.append({\n",
    "            \"sampler\": sampler_name,\n",
    "            \"model\": model_name,\n",
    "            **metrics\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae875304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          sampler   model    PR_AUC   ROC_AUC\n",
      "4           tomek      rf  0.798064  0.993175\n",
      "5             enn      rf  0.784224  0.994010\n",
      "1  cost_sensitive      rf  0.776676  0.993588\n",
      "0  cost_sensitive     xgb  0.721121  0.991970\n",
      "3           tomek     xgb  0.716431  0.991043\n",
      "7  cost_sensitive     mlp  0.600800  0.966482\n",
      "2  cost_sensitive  logreg  0.123529  0.853643\n",
      "6       smote_enn  logreg  0.108587  0.839485\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results).sort_values(\"PR_AUC\", ascending=False)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4da40e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Create a results list with precision and recall\n",
    "results_with_pr = []\n",
    "\n",
    "# Use a standard threshold 0.5 for now\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "for entry in results:\n",
    "    sampler_name = entry[\"sampler\"]\n",
    "    model_name = entry[\"model\"]\n",
    "\n",
    "    # Retrieve the fitted pipeline/model from MLflow or previous loop\n",
    "    # Assuming you kept `fitted_model` for each combination\n",
    "    pipeline_or_model = entry.get(\"fitted_model\")  # If you stored it\n",
    "\n",
    "    if pipeline_or_model is None:\n",
    "        continue  # skip if model not stored\n",
    "\n",
    "    # Predict probabilities\n",
    "    y_prob = pipeline_or_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Convert to class predictions using threshold\n",
    "    y_pred = (y_prob >= THRESHOLD).astype(int)\n",
    "\n",
    "    # Compute precision and recall\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec  = recall_score(y_test, y_pred)\n",
    "\n",
    "    results_with_pr.append({\n",
    "        \"sampler\": sampler_name,\n",
    "        \"model\": model_name,\n",
    "        \"PR_AUC\": entry[\"PR_AUC\"],\n",
    "        \"ROC_AUC\": entry[\"ROC_AUC\"],\n",
    "        \"Precision\": prec,\n",
    "        \"Recall\": rec\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_pr_df = pd.DataFrame(results_with_pr).sort_values(\"PR_AUC\", ascending=False)\n",
    "print(results_pr_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d950b6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "df = pd.read_csv('cc_data.csv')\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'])\n",
    "df['dob'] = pd.to_datetime(df['dob'])\n",
    "\n",
    "# Sort by customer and time\n",
    "df = df.sort_values(by=['cc_num', 'trans_date_trans_time']).reset_index(drop=True)\n",
    "\n",
    "# Time features\n",
    "df['transaction_hour'] = df['trans_date_trans_time'].dt.hour\n",
    "df['transaction_day'] = df['trans_date_trans_time'].dt.day\n",
    "df['transaction_month'] = df['trans_date_trans_time'].dt.month\n",
    "df['transaction_weekday'] = df['trans_date_trans_time'].dt.weekday\n",
    "df['is_weekend'] = df['transaction_weekday'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Age\n",
    "df['age'] = (df['trans_date_trans_time'] - df['dob']).dt.days / 365.25\n",
    "\n",
    "# -------------------------------\n",
    "# Rolling transaction counts\n",
    "# -------------------------------\n",
    "df.set_index('trans_date_trans_time', inplace=True)\n",
    "df['txn_count_1h'] = df.groupby('cc_num')['amt'].rolling('1h').count().reset_index(level=0, drop=True)\n",
    "df['txn_count_24h'] = df.groupby('cc_num')['amt'].rolling('24h').count().reset_index(level=0, drop=True)\n",
    "\n",
    "# Rolling amount statistics (24h, no leakage)\n",
    "df['amt_mean_24h'] = df.groupby('cc_num')['amt'].rolling('24h', closed='left').mean().reset_index(level=0, drop=True)\n",
    "df['amt_std_24h'] = df.groupby('cc_num')['amt'].rolling('24h', closed='left').std().reset_index(level=0, drop=True)\n",
    "df['amt_zscore_24h'] = (df['amt'] - df['amt_mean_24h']) / (df['amt_std_24h'] + 1e-6)\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "# -------------------------------\n",
    "# Geodistance\n",
    "# -------------------------------\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2)**2\n",
    "    c = 2*np.arcsin(np.sqrt(a))\n",
    "    return 6371 * c\n",
    "\n",
    "df['geo_distance_km'] = haversine(df['lat'], df['long'], df['merch_lat'], df['merch_long'])\n",
    "\n",
    "# -------------------------------\n",
    "# Time-based train/test split (no leakage)\n",
    "# -------------------------------\n",
    "df = df.sort_values('trans_date_trans_time')\n",
    "split_time = df['trans_date_trans_time'].quantile(0.8)\n",
    "train_df = df[df['trans_date_trans_time'] <= split_time].copy()\n",
    "test_df  = df[df['trans_date_trans_time'] > split_time].copy()\n",
    "\n",
    "# -------------------------------\n",
    "# 6 Handle missing values safely\n",
    "# -------------------------------\n",
    "cols_to_process = ['amt_mean_24h', 'amt_std_24h', 'amt_zscore_24h']\n",
    "for col in cols_to_process:\n",
    "    train_df[f'{col}_missing'] = train_df[col].isna().astype(int)\n",
    "    test_df[f'{col}_missing'] = test_df[col].isna().astype(int)\n",
    "    median = train_df[col].median()  # Use train median only\n",
    "    train_df[col] = train_df[col].fillna(median)\n",
    "    test_df[col] = test_df[col].fillna(median)\n",
    "\n",
    "# -------------------------------\n",
    "# Risk encoding (train-only, no leakage)\n",
    "# -------------------------------\n",
    "global_fraud_rate = train_df['is_fraud'].mean()\n",
    "\n",
    "def risk_encode(train_df, test_df, col, target='is_fraud', min_samples=50):\n",
    "    stats = train_df.groupby(col)[target].agg(['mean','count']).rename(columns={'mean':'fraudrate','count':'n'})\n",
    "    stats['risk'] = (stats['fraudrate']*stats['n'] + global_fraud_rate*min_samples) / (stats['n']+min_samples)\n",
    "    train_encoded = train_df[col].map(stats['risk']).fillna(global_fraud_rate)\n",
    "    test_encoded  = test_df[col].map(stats['risk']).fillna(global_fraud_rate)\n",
    "    return train_encoded, test_encoded\n",
    "\n",
    "for col in ['merchant','category','job']:\n",
    "    train_df[f'{col}_risk'], test_df[f'{col}_risk'] = risk_encode(train_df, test_df, col)\n",
    "\n",
    "# -------------------------------\n",
    "# 8 Prepare final features\n",
    "# -------------------------------\n",
    "FINAL_FEATURES = [\n",
    "    'amt', 'gender', 'city_pop', 'age', 'transaction_hour', 'transaction_day',\n",
    "    'transaction_month', 'transaction_weekday', 'is_weekend',\n",
    "    'txn_count_1h', 'txn_count_24h', 'amt_mean_24h', 'amt_std_24h', 'amt_zscore_24h',\n",
    "    'amt_mean_24h_missing', 'amt_std_24h_missing', 'amt_zscore_24h_missing',\n",
    "    'geo_distance_km', 'merchant_risk', 'category_risk', 'job_risk',\n",
    "    'lat', 'long', 'merch_lat', 'merch_long'\n",
    "]\n",
    "\n",
    "\n",
    "X_train = train_df[FINAL_FEATURES].copy()\n",
    "y_train = train_df['is_fraud']\n",
    "X_test  = test_df[FINAL_FEATURES].copy()\n",
    "y_test  = test_df['is_fraud']\n",
    "\n",
    "# -------------------------------\n",
    "# Encode gender\n",
    "# -------------------------------\n",
    "X_train['gender'] = X_train['gender'].str.upper().map({'M':1,'F':0}).fillna(-1).astype(int)\n",
    "X_test['gender']  = X_test['gender'].str.upper().map({'M':1,'F':0}).fillna(-1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "623f3794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as VIKR4NT10\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Accessing as VIKR4NT10\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"VIKR4NT10/codesoft\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"VIKR4NT10/codesoft\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository VIKR4NT10/codesoft initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository VIKR4NT10/codesoft initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/e38178a5cb324196a652f5d18b11e664', creation_time=1768754683163, experiment_id='6', last_update_time=1768754683163, lifecycle_stage='active', name='hyperparameter-tuning for credit-card fraud detection', tags={}>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "import dagshub\n",
    "# ========================== CONFIG ==========================\n",
    "CONFIG = {\n",
    "    \"experiment_name\": \"hyperparameter-tuning for credit-card fraud detection\",\n",
    "    \"mlflow_uri\": \"https://dagshub.com/VIKR4NT10/codesoft.mlflow\",\n",
    "    \"repo_owner\": \"VIKR4NT10\",\n",
    "    \"repo_name\": \"codesoft\"\n",
    "}\n",
    "\n",
    "# ========================== MLflow + DAGsHub ==========================\n",
    "mlflow.set_tracking_uri(CONFIG[\"mlflow_uri\"])\n",
    "dagshub.init(\n",
    "    repo_owner=CONFIG[\"repo_owner\"],\n",
    "    repo_name=CONFIG[\"repo_name\"],\n",
    "    mlflow=True\n",
    ")\n",
    "mlflow.set_experiment(CONFIG[\"experiment_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b831be10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 15 candidates, totalling 45 fits\n",
      "üèÉ View run RF_SMOTETomek_RandomSearch at: https://dagshub.com/VIKR4NT10/codesoft.mlflow/#/experiments/6/runs/a3e0da395f644921b060efabb9ff7cf4\n",
      "üß™ View experiment at: https://dagshub.com/VIKR4NT10/codesoft.mlflow/#/experiments/6\n",
      "Best PR-AUC (CV): 0.8378649542477078\n",
      "Best parameters:\n",
      "  rf__n_estimators: 200\n",
      "  rf__min_samples_leaf: 20\n",
      "  rf__max_features: 0.5\n",
      "  rf__max_depth: None\n",
      "  rf__bootstrap: False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import average_precision_score\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline\n",
    "import mlflow\n",
    "\n",
    "# -------------------------\n",
    "# 1. Subsample training data (10%)\n",
    "# -------------------------\n",
    "FRAC = 0.1\n",
    "\n",
    "X_train_sub, _, y_train_sub, _ = train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    train_size=FRAC,\n",
    "    stratify=y_train,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 2. Define pipeline\n",
    "# -------------------------\n",
    "pipeline = Pipeline([\n",
    "    (\"sampler\", SMOTETomek(random_state=42)),\n",
    "    (\"rf\", RandomForestClassifier(\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        class_weight=None  # handled by sampling\n",
    "    ))\n",
    "])\n",
    "\n",
    "# -------------------------\n",
    "# 3. Hyperparameter search space\n",
    "# -------------------------\n",
    "param_dist = {\n",
    "    \"rf__n_estimators\": [100, 200, 300],\n",
    "    \"rf__max_depth\": [None, 8, 12, 16],\n",
    "    \"rf__min_samples_leaf\": [20, 50, 100],\n",
    "    \"rf__max_features\": [\"sqrt\", \"log2\", 0.3, 0.5],\n",
    "    \"rf__bootstrap\": [True, False]\n",
    "}\n",
    "\n",
    "# -------------------------\n",
    "# 4. Cross-validation strategy\n",
    "# -------------------------\n",
    "cv = StratifiedKFold(\n",
    "    n_splits=3,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 5. RandomizedSearchCV\n",
    "# -------------------------\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=15,                       # safe for Colab\n",
    "    scoring=\"average_precision\",     # PR-AUC\n",
    "    cv=cv,\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 6. Run hyperparameter tuning\n",
    "# -------------------------\n",
    "with mlflow.start_run(run_name=\"RF_SMOTETomek_RandomSearch\"):\n",
    "    search.fit(X_train_sub, y_train_sub)\n",
    "\n",
    "    mlflow.log_params(search.best_params_)\n",
    "    mlflow.log_metric(\"best_cv_pr_auc\", search.best_score_)\n",
    "\n",
    "# -------------------------\n",
    "# 7. Output best results\n",
    "# -------------------------\n",
    "print(\"Best PR-AUC (CV):\", search.best_score_)\n",
    "print(\"Best parameters:\")\n",
    "for k, v in search.best_params_.items():\n",
    "    print(f\"  {k}: {v}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "titan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
