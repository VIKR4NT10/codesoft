{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ffa808c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"spam.csv\", encoding=\"latin-1\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed3bb51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Comparison Results:\n",
      "\n",
      "                 Model  Accuracy  Precision    Recall  F1 Score\n",
      "2           Linear SVM  0.983857   0.992481  0.885906  0.936170\n",
      "3        Random Forest  0.974888   1.000000  0.812081  0.896296\n",
      "0          Naive Bayes  0.968610   1.000000  0.765101  0.866920\n",
      "1  Logistic Regression  0.967713   1.000000  0.758389  0.862595\n",
      "4    Gradient Boosting  0.961435   0.990741  0.718121  0.832685\n"
     ]
    }
   ],
   "source": [
    "df = df[[\"v1\", \"v2\"]]\n",
    "df = df.rename(columns={\"v1\": \"label\", \"v2\": \"text\"})\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# ----------------------------\n",
    "# spam -> 1, ham -> 0\n",
    "df[\"label\"] = df[\"label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Train-test split\n",
    "# ----------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"text\"],\n",
    "    df[\"label\"],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Models to compare\n",
    "# ----------------------------\n",
    "models = {\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Linear SVM\": LinearSVC(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Train & Evaluate\n",
    "# ----------------------------\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipeline = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(stop_words=\"english\")),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"Precision\": precision_score(y_test, y_pred),\n",
    "        \"Recall\": recall_score(y_test, y_pred),\n",
    "        \"F1 Score\": f1_score(y_test, y_pred)\n",
    "    })\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Display results\n",
    "# ----------------------------\n",
    "results_df = pd.DataFrame(results).sort_values(by=\"F1 Score\", ascending=False)\n",
    "\n",
    "print(\"\\nModel Comparison Results:\\n\")\n",
    "print(results_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4383494",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\.conda\\envs\\titan\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Dense NN...\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\n",
      "Training CNN...\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "\n",
      "Training LSTM...\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step\n",
      "\n",
      "Training Bi-LSTM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\.conda\\envs\\titan\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 101ms/step\n",
      "\n",
      "Deep Learning Model Comparison:\n",
      "\n",
      "      Model  Accuracy  Precision    Recall  F1 Score\n",
      "1       CNN  0.988341   0.972222  0.939597  0.955631\n",
      "0  Dense NN  0.988341   1.000000  0.912752  0.954386\n",
      "3   Bi-LSTM  0.988341   1.000000  0.912752  0.954386\n",
      "2      LSTM  0.866368   0.000000  0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Embedding, LSTM, Bidirectional,\n",
    "    Conv1D, GlobalMaxPooling1D, Dropout\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Tokenization & Padding\n",
    "# ----------------------------\n",
    "VOCAB_SIZE = 10000\n",
    "MAX_LEN = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding=\"post\")\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding=\"post\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Model builders\n",
    "# ----------------------------\n",
    "def dense_model():\n",
    "    model = Sequential([\n",
    "        Embedding(VOCAB_SIZE, 128, input_length=MAX_LEN),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def cnn_model():\n",
    "    model = Sequential([\n",
    "        Embedding(VOCAB_SIZE, 128, input_length=MAX_LEN),\n",
    "        Conv1D(128, 5, activation=\"relu\"),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def lstm_model():\n",
    "    model = Sequential([\n",
    "        Embedding(VOCAB_SIZE, 128, input_length=MAX_LEN),\n",
    "        LSTM(128),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def bi_lstm_model():\n",
    "    model = Sequential([\n",
    "        Embedding(VOCAB_SIZE, 128, input_length=MAX_LEN),\n",
    "        Bidirectional(LSTM(128)),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Train & Evaluate\n",
    "# ----------------------------\n",
    "models = {\n",
    "    \"Dense NN\": dense_model(),\n",
    "    \"CNN\": cnn_model(),\n",
    "    \"LSTM\": lstm_model(),\n",
    "    \"Bi-LSTM\": bi_lstm_model()\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=\"adam\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train_pad, y_train,\n",
    "        epochs=5,\n",
    "        batch_size=32,\n",
    "        validation_split=0.1,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    y_pred = (model.predict(X_test_pad) > 0.5).astype(int)\n",
    "\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"Precision\": precision_score(y_test, y_pred),\n",
    "        \"Recall\": recall_score(y_test, y_pred),\n",
    "        \"F1 Score\": f1_score(y_test, y_pred)\n",
    "    })\n",
    "\n",
    "# ----------------------------\n",
    "# 6. Results\n",
    "# ----------------------------\n",
    "results_df = pd.DataFrame(results).sort_values(by=\"F1 Score\", ascending=False)\n",
    "print(\"\\nDeep Learning Model Comparison:\\n\")\n",
    "print(results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "titan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
